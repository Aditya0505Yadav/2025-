{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya0505Yadav/2025-/blob/main/Prompting_with_API%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applied Generative AI\n",
        "Instructor: Prof. Dehghani\n",
        "Welcome to the Applied Generative AI course. In this course, we will explore the foundations and applications of Generative AI using tools like OpenAI's API. By the end of this session, you will:\n",
        "\n",
        "üü¢ Understand how to set up and connect to OpenAI's API.\n",
        "üåü Learn about the roles (System, Assistant, User) in prompt design.\n",
        "‚ú® Generate text, images, and vector embeddings programmatically.\n",
        "üîß Explore fine-tuning to customize AI models for specific tasks.\n",
        "Let‚Äôs get started with setting up the OpenAI API!"
      ],
      "metadata": {
        "id": "3S5eQNmwPUH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI Python SDK\n",
        "# This library allows us to interact with OpenAI's API for text, images, and embeddings.\n",
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "WNdB6dNqWG7L",
        "outputId": "9ef8420c-d576-4ff7-9e00-8195f1462e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.76.2\n",
            "    Uninstalling openai-1.76.2:\n",
            "      Successfully uninstalled openai-1.76.2\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Retrieve the key from Colab secrets\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Confirm the key was loaded (for debugging only; don‚Äôt print your real key in shared notebooks!)\n",
        "print(\"Key loaded:\", \"Yes\" if openai.api_key else \"No\")\n"
      ],
      "metadata": {
        "id": "IkCBK7FpWJ4e",
        "outputId": "affba048-992c-4434-cbc2-75e9af99364d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key loaded: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Prompt Playground: Understanding Roles\n",
        "In OpenAI's API, you interact with the model using roles, which define the flow of the conversation:\n",
        "\n",
        "‚û°Ô∏è System: Sets the behavior and tone of the assistant (e.g., \"You are a cheerful assistant.\").\n",
        "‚û°Ô∏è User: Represents the input or question from the user (e.g., \"What is AI?\").\n",
        "‚û°Ô∏è Assistant: Automatically generated responses based on the system and user inputs.\n",
        "üí° Why Roles Matter: Roles help control the assistant's personality and the quality of responses. For example:\n",
        "\n",
        "A system message like \"You are a strict teacher\" makes the assistant respond more formally.\n",
        "A system message like \"You are a friendly chatbot\" leads to casual responses.\n",
        "Let‚Äôs see how these roles work in the next cell!\n",
        "\n",
        "## Example 1: Without Assistant Role"
      ],
      "metadata": {
        "id": "Rtir4dZMPrc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "id": "YXvUTgLCWq4u",
        "outputId": "14a1b4c4-a94a-44e8-8516-fbf2057a317d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: With Assistant Role"
      ],
      "metadata": {
        "id": "ytKbu3esTO5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating roles in OpenAI's API with the assistant role\n",
        "\n",
        "# Define the conversation including a predefined assistant response\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor who explains problems step by step.\"},  # System role sets the behavior\n",
        "    {\"role\": \"user\", \"content\": \"Solve for x: 2x + 5 = 15\"},  # User question\n",
        "    {\"role\": \"assistant\", \"content\": \"To solve for x: \\n1. Subtract 5 from both sides: 2x = 10\\n2. Divide both sides by 2: x = 5\"}  # Predefined assistant response\n",
        "]\n",
        "\n",
        "# Send the conversation to the API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",  # Use the chosen model\n",
        "    messages=messages  # Pass the conversation\n",
        ")\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "9pcHetiWWuM-",
        "outputId": "4d428f83-eb87-4559-f8da-e12e60ff4ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant's Response: Let's solve the equation 2x + 5 = 15 step by step for x.\n",
            "\n",
            "Step 1: To get the 'x' term alone on one side of the equation, we first have to remove the '+5' term from the left-hand side. We can do that by subtracting '5' from both sides of the equation. That gives us:\n",
            "\n",
            "2x + 5 - 5 = 15 - 5 \n",
            "\n",
            "which simplifies to:\n",
            "\n",
            "2x = 10 \n",
            "\n",
            "Step 2: We now see that 'x' is multiplied by 2 on the left-hand side. To solve for 'x', we have to remove this multiplication. We do that by dividing both sides of the equation by 2. That gives us:\n",
            "\n",
            "2x/2 = 10/2 \n",
            "\n",
            "which simplifies to:\n",
            "\n",
            "x = 5\n",
            "\n",
            "So the solution is x = 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üßÆ Hands-On"
      ],
      "metadata": {
        "id": "BsL0AR6MTajz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++ Hands-On: Complete the Roles ++++\n",
        "\n",
        "# Define the conversation using the role structure\n",
        "messages = [\n",
        "    {\"role\": \"----\", \"content\": \"You are a math tutor who explains concepts clearly and step by step.\"},  # Complete the system role\n",
        "    {\"role\": \"----\", \"content\": \"How do you calculate the arithmetic mean of a set of numbers?\"},  # Complete the user role\n",
        "    {\"role\": \"----\", \"content\": \"To calculate the arithmetic mean:\\n\"\n",
        "                                \"1. Add all the numbers in the set.\\n\"\n",
        "                                \"2. Divide the sum by the number of numbers.\\n\\n\"\n",
        "                                \"For example, for 10, 20, and 30:\\n\"\n",
        "                                \"Mean = (10 + 20 + 30) / 3 = 60 / 3 = 20.\"\n",
        "                                }  # Optional predefined assistant response\n",
        "]\n",
        "\n",
        "# Uncomment the code below after completing the placeholders\n",
        "# response = openai.ChatCompletion.create(\n",
        "#     model=\"gpt-4\",  # Specify the model\n",
        "#     messages=messages  # Pass the completed conversation\n",
        "# )\n",
        "# print(\"Assistant's Response:\", response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "Bl5CCoC9TZ1z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Simple Q&A with System + User Roles\n",
        "üëâ Task:\n",
        "Write a prompt that asks the assistant to behave like a science teacher, then ask it a science-related question."
      ],
      "metadata": {
        "id": "bpTm-F37UIMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Define your own roles and prompt below\n",
        "messages = [\n",
        "    {\"role\": \"------\", \"content\": \"You are a [insert role/personality, e.g., 'science teacher', 'debate coach'].\"},\n",
        "    {\"role\": \"------\", \"content\": \"Ask your question here, e.g., 'Explain how photosynthesis works.'\"}\n",
        "]\n",
        "\n",
        "# üß† Call the OpenAI API with your customized prompt\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# üß† Print the model's response\n",
        "print(\"Assistant's Response:\", response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "_ZNQimnSUIpr",
        "outputId": "0082ac86-2c95-4e40-a084-b619204da745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "Invalid value: '------'. Supported values are: 'system', 'assistant', 'user', 'function', 'tool', and 'developer'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a81f68e5f08b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# üß† Call the OpenAI API with your customized prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid value: '------'. Supported values are: 'system', 'assistant', 'user', 'function', 'tool', and 'developer'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úçÔ∏è Exercise 2: Give a prompt of your choice.\n",
        "üëâ Task:\n",
        "Now write your own custom prompt. Use any role, any question ‚Äî be creative!"
      ],
      "metadata": {
        "id": "bNOSyWlbUs82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your own prompt idea!\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a ____.\"},\n",
        "    {\"role\": \"user\", \"content\": \"____?\"}\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(\"Assistant's Response:\", response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "id": "frrq5UkhU8Xj",
        "outputId": "fbc008dc-3cd6-4b8b-d8be-6cf782562797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant's Response: I'm an artificial intelligence created by OpenAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIJh9B4Zy6Ll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}